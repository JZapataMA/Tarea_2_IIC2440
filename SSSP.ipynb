{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalamos PySpark\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "rdd_nodes = spark.sparkContext.parallelize(nodes)\n",
    "rdd_edges = spark.sparkContext.parallelize(edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos nodos y aristas aleatorias para usarlo en el algoritmo\n",
    "import random\n",
    "\n",
    "numero_nodos = 20\n",
    "\n",
    "numero_aristas = 50\n",
    "\n",
    "nodes = [int(i) for i in range(numero_nodos)]\n",
    "\n",
    "edges = []\n",
    "for _ in range(numero_aristas):\n",
    "    nodo_inicial = random.randint(0, numero_nodos)\n",
    "    nodo_destino = random.randint(0, numero_nodos)\n",
    "    peso = random.randint(1, 40)\n",
    "    arista = (nodo_inicial, nodo_destino, peso)\n",
    "    edges.append(arista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes)\n",
    "print(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paralelizamos\n",
    "rdd_nodes = spark.sparkContext.parallelize(nodes)\n",
    "rdd_edges = spark.sparkContext.parallelize(edges)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SELECCIÓN DEL NODO INICIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos un nodo incial (Tener ojo con la seleccion, si por algun motivo este nodo no esta conectado a ningun otro, va a dar infinito en todos los caminos, debido\n",
    "# a que no existe un camino entre ese nodo y otro)\n",
    "nodo_inicial = 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtramos los caminos que salen de ese nodo\n",
    "rdd_caminos_iniciales = rdd_edges.filter(lambda x: x[0] == nodo_inicial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicialicemos los costes de los nodos a infinito y el coste del nodo origen a 0\n",
    "rdd_costs = rdd_nodes.map(lambda x: (x, float('inf')))\n",
    "rdd_costs = rdd_costs.map(lambda x: (x[0], 0.0) if x[0] == nodo_inicial else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# realicemos lo anterior de manera iterativa hasta que no existan caminos que mejoren el coste de llegada a los nodos destino\n",
    "# creemos un ciclo que se repita hasta que no existan caminos que mejoren el coste de llegada a los nodos destino\n",
    "while True:\n",
    "    # veamos los caminos que encontramos desde el nodo_destino de los caminos iniciales\n",
    "\n",
    "    rdd_costs_ac = rdd_edges.map(lambda x: (x[0],(x))).join(rdd_caminos_iniciales.map(lambda x: (x[1], (x[0],x[1],x[2]))))\n",
    "    rdd_caminos = rdd_caminos_iniciales.union(rdd_costs_ac.map(lambda x: (x[1][1][0],x[1][0][1],x[1][1][2] + x[1][0][2])))\n",
    "\n",
    "    # actualicemos los costes de los nodos destino si el costo del camino encontrado es menor que el costo del nodo destino\n",
    "    # unamos los rdd y luego hagamos un reduceByKey para quedarnos con el menor coste\n",
    "\n",
    "    rdd_costs_nuevos = rdd_costs.union(rdd_caminos.map(lambda x: (x[1], x[2])))\n",
    "    rdd_costs_nuevos = rdd_costs_nuevos.reduceByKey(lambda x,y: x if x < y else y)\n",
    "\n",
    "    # veamos si los costes encontrados son iguales a los ya encontrados\n",
    "    if rdd_costs_nuevos.join(rdd_costs).filter(lambda x: x[1][0] != x[1][1]).count() == 0:\n",
    "        break\n",
    "\n",
    "    # actualicemos los costes iniciales con los costes encontrados\n",
    "    rdd_costs = rdd_costs_nuevos\n",
    "    # actualicemos los caminos iniciales con los caminos encontrados\n",
    "    rdd_caminos_iniciales = rdd_caminos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_costs.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Una consideración\n",
    "\n",
    "Lo llegamos a escalar con 70 nodos y como 150 aristas, corría el programa y todo, se demoraba su rato piola, pero de que funciona, funciona "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geopandas_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
