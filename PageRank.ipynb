{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalamos PySpark\n",
    "!pip install pyspark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTAMOS DESDE DRIVE (CAMBIAR LA BASE DE DATOS SI ES NECESARIA [PORFAVOR RESPETAR LA ESTRUCTURA O ME PONDRE TRISTE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En este caso, nosotros tenemos cora en nuestro google drive, ustedes deben poner su ruta para cora.cites\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "import pandas as pd\n",
    "# En este caso, nosotros tenemos cora en nuestro google drive, ustedes deben poner su ruta para cora.cites\n",
    "df = pd.read_csv('/content/drive/MyDrive/cora.cites',sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"target\", \"source\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasamos el df anterior a listas para procesarlos en nuestro algoritmo.\n",
    "\n",
    "# Obtener edges\n",
    "edges = list(zip(df['source'], df['target']))\n",
    "\n",
    "# Obtener nodos\n",
    "nodes = list(set(df['source']).union(set(df['target'])))\n",
    "\n",
    "print(edges)\n",
    "print(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_nodes = spark.sparkContext.parallelize(nodes)\n",
    "rdd_edges = spark.sparkContext.parallelize(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta funcion se encarga de asignar el PageRank inicial para cada nodo\n",
    "def asignar_pagerank(nodo, cantidad_nodos):\n",
    "    return (nodo, 1 / cantidad_nodos)\n",
    "\n",
    "\n",
    "cantidad_nodos = rdd_nodes.count()\n",
    "rdd_nodes = rdd_nodes.map(lambda nodo: asignar_pagerank(nodo, cantidad_nodos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteo se encarga de contar las aristas asociadas a un nodo (Solo aristas que salen desde el nodo, no las que llegan al nodo)\n",
    "conteo = rdd_edges.map(lambda x: (x[0], 1)).reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los mensajes corresponden a tuplas las cuales tienen la siguiente forma (nodo, mensajes_recibidos)\n",
    "\n",
    "def mensajes_inicialesrdd(rdd_nodes,conteo):\n",
    "  rdd_salidas = rdd_nodes.map(lambda x: (x[0], x[1])).join(conteo)\n",
    "  rdd_salidas = rdd_salidas.map(lambda tupla: (tupla[0],tupla[1][0]/tupla[1][1]))\n",
    "  return rdd_salidas\n",
    "\n",
    "rdd_mensajes = mensajes_inicialesrdd(rdd_nodes,conteo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos un Join entre los mensajes y las aristas\n",
    "join_rdds = rdd_mensajes.join(rdd_edges).map(lambda line: (line[1][1], line[1][0])) \\\n",
    "            .reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rellenamos aquellos nodos que no se incluyeron debido a que no reciben mensajes\n",
    "nodes_with_zero = rdd_nodes.map(lambda x: (x[0], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un RDD final para trabajar en el algoritmo\n",
    "rdd_final = join_rdds.union(nodes_with_zero).reduceByKey(lambda a, b: a + b).map(lambda x: (x[0], 1/4, x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nuestro dumping factor va a ser considerado como 0.85\n",
    "\n",
    "# Define el factor de dumping\n",
    "dumping_factor = 0.85\n",
    "\n",
    "# Calcula el número total de nodos en el RDD\n",
    "num_nodos = rdd_final.count()\n",
    "val_inicial = 1 / num_nodos\n",
    "# Calcula el factor de redistribución\n",
    "redistribution_factor = (1 - dumping_factor) / num_nodos\n",
    "\n",
    "# Función para actualizar los valores de PageRank para cada nodo\n",
    "def actualizar_page_rank(nodo):\n",
    "    nodo_id, page_rank, ref = nodo\n",
    "    page_rank = (1 - dumping_factor)/num_nodos\n",
    "    new_page_rank =  page_rank + (dumping_factor * ref)\n",
    "    return (nodo_id, new_page_rank, (dumping_factor * ref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm es un algoritmo que utiliza actulizar_page_rank\n",
    "def algorithm(rdd_final):\n",
    "\n",
    "  dumping_factor = 0.85\n",
    "  num_nodos = rdd_final.count()\n",
    "  redistribution_factor = (1 - dumping_factor) / num_nodos\n",
    "  rdd_actualizado = rdd_final.map(actualizar_page_rank)\n",
    "\n",
    "  return rdd_actualizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una iteracion maxima para terminar, en este caso es 10, o su cambio es menor a epsilon.\n",
    "# El print lo hace del tipo (nodo, PageRank, Redistribution Factor)\n",
    "\n",
    "count = 0\n",
    "epsilon = 0.0000001\n",
    "iteracion_anterior = None\n",
    "while True:\n",
    "    rdd_final = algorithm(rdd_final)\n",
    "    if iteracion_anterior is not None:\n",
    "        # veamos si los valores de la iteracion anterior y la actual son muy parecidos\n",
    "        if iteracion_anterior.join(rdd_final).filter(lambda x: x[1][0] !=x [1][1]).filter(lambda x: abs(x[1][0] - x[1][1]) < epsilon).count() != 0:\n",
    "            break\n",
    "    print(rdd_final.collect())\n",
    "    count += 1\n",
    "    iteracion_anterior = rdd_final\n",
    "    if count == 10:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
